# evaluation/run_evaluation.py
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_correctness,
)
import os
import ast

# --- CONFIGURATION ---
# Ensure your OPENAI_API_KEY is set as an environment variable for the RAGAS evaluator LLM
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OPENAI_API_KEY must be set for RAGAS evaluation.")

RESULTS_FILE = "evaluation/generated_results_hyde.csv"
SCORES_FILE = "evaluation/ragas_scores_hyde.csv"

# --- MAIN SCRIPT ---
if __name__ == "__main__":
    # Load the results generated by your agent
    try:
        results_df = pd.read_csv(RESULTS_FILE)
    except FileNotFoundError:
        print(f"Error: `{RESULTS_FILE}` not found. Please run the generation script first.")
        exit()

    # The 'contexts' column is saved as a string representation of a list.
    # We need to convert it back to an actual list.
    results_df['contexts'] = results_df['contexts'].apply(ast.literal_eval)
    
    # Convert to Hugging Face Dataset format, which RAGAS expects
    hf_dataset = Dataset.from_pandas(results_df)

    # Define the metrics you want to calculate
    metrics = [
        faithfulness,          # How much is the answer grounded in the context?
        answer_relevancy,      # How relevant is the answer to the question?
        context_precision,     # Are the retrieved contexts relevant? (Signal)
        context_recall,        # Was all the necessary context retrieved? (Noise)
        answer_correctness,    # How factually correct is the answer compared to the ground truth?
    ]
    
    print(f"Running RAGAS evaluation on '{RESULTS_FILE}'...")
    # Run the evaluation
    result = evaluate(
        dataset=hf_dataset,
        metrics=metrics,
    )
    
    # Display and save the results
    evaluation_df = result.to_pandas()
    
    print("\n--- Evaluation Complete ---")
    print(evaluation_df)
    
    evaluation_df.to_csv(SCORES_FILE, index=False)
    print(f"\nâœ… Evaluation scores saved to {SCORES_FILE}")

    # Print a summary grouped by framework
    summary = evaluation_df.groupby('framework')[['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'answer_correctness']].mean()
    print("\n--- Average Scores by Framework ---")
    print(summary)




    